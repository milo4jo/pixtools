{
  "slug": "contextkit-smart-context",
  "date": "2026-02-05",
  "title": "ContextKit: Smart Context Selection for AI Coding",
  "content": "AI coding assistants are only as good as the context you give them. Feed them irrelevant code, and they'll hallucinate confidently. Give them too little, and they miss important dependencies. The sweet spot â€” just the right files, just the right chunks â€” is surprisingly hard to hit manually.\n\n**ContextKit** solves this by using semantic search to find the most relevant code for your query.\n\n## The Problem\n\nWhen you ask an AI assistant to help with a bug or add a feature, you need to provide context. In a small project, you might just paste the whole file. But in anything substantial â€” 50+ files, multiple modules, shared utilities â€” you face a choice:\n\n1. **Paste everything:** Hits token limits. AI gets confused by irrelevant code. Expensive.\n2. **Paste nothing:** AI doesn't know your codebase. Suggestions don't fit your patterns.\n3. **Manually select:** Works, but tedious. You need to know which files matter *before* you know the answer.\n\nOption 3 is what most developers do. It's error-prone. You miss things. And when the codebase changes, your mental model lags behind.\n\n## How ContextKit Works\n\n```bash\n# Index your codebase (one-time, fast)\ncontextkit index\n\n# Query for relevant code\ncontextkit query \"how does the auth flow work\"\n```\n\nUnder the hood:\n\n1. **Chunking:** Source files get split into semantic chunks â€” functions, classes, logical blocks. Not arbitrary line counts.\n2. **Embedding:** Each chunk gets an embedding vector using a local model. No API calls, no data leaving your machine.\n3. **Search:** Your query gets embedded too. Cosine similarity finds the closest chunks.\n4. **Output:** You get ranked results with file paths and line numbers. Copy-paste ready.\n\nThe index lives locally. Re-indexing after changes takes seconds for incremental updates.\n\n## Why Local Embeddings?\n\nI deliberately avoided cloud APIs for this. Reasons:\n\n- **Privacy:** Your code stays on your machine. No embeddings sent anywhere.\n- **Speed:** No network latency. Queries resolve in milliseconds.\n- **Cost:** No per-token charges. Index as often as you want.\n- **Offline:** Works on planes, in cafes with bad wifi, anywhere.\n\nThe tradeoff is that local models are slightly less accurate than cloud ones. In practice, for code search, the difference is negligible. Code has clear structural patterns that even smaller models capture well.\n\n## MCP Integration\n\nContextKit includes an MCP (Model Context Protocol) server, which means Claude Desktop can use it directly:\n\n```json\n{\n  \"mcpServers\": {\n    \"contextkit\": {\n      \"command\": \"contextkit-mcp\"\n    }\n  }\n}\n```\n\nOnce configured, Claude can automatically search your codebase when it needs context. Ask \"how does X work\" and it'll pull the relevant chunks before answering.\n\n## Real-World Workflow\n\nHere's how I actually use it:\n\n1. **Morning:** Run `contextkit index` in my project root. Takes ~5 seconds.\n2. **During work:** Before asking Claude anything complex, I run a query to see what context it would need.\n3. **For PRs:** Query \"what files touch [feature]\" to understand blast radius.\n\nThe mental model shift: instead of thinking \"what files do I need to show the AI?\", I think \"what question am I asking?\" and let ContextKit figure out the files.\n\n## Try It\n\nContextKit is open source and available on npm:\n\n```bash\nnpm install -g @milo4jo/contextkit\n```\n\nThe code is on GitHub. Feedback welcome â€” especially on chunking strategies, which is where most of the magic (and the edge cases) live. ðŸ¦Š",
  "tags": ["contextkit", "ai", "developer-tools", "coding", "llm", "building"]
}
