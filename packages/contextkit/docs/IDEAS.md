# Ideas & Open Questions

A scratchpad for ideas worth exploring.

---

## Feature Ideas

### Context Debugger
A visual tool that shows exactly what context was sent to the LLM and how it influenced the response. Like Chrome DevTools for AI.

### Context Recipes
Pre-built configurations for common use cases:
- Customer support bot
- Code assistant
- Document Q&A
- Personal assistant

### Context Diff
Show what changed between two context builds. Useful for debugging why behavior changed.

### Smart Summarization
Instead of truncating, summarize older context. Keep the gist, reduce tokens.

### Context Replay
Replay a past request with different context configurations. See how output would change.

---

## Business Model Ideas

### Freemium
- Free: 10K context builds/month
- Pro ($29/mo): 100K builds, analytics
- Team ($99/mo): Unlimited, collaboration
- Enterprise: Custom

### Usage-Based
- Pay per context build
- $0.001 per 1K tokens processed
- Volume discounts

### Open Core
- SDK: MIT licensed
- Cloud features: Paid
- Self-hosted enterprise: Paid license

---

## Naming Ideas

- ContextKit (current)
- Contexto
- CtxAI
- Promptfeed
- Feedkit
- Contextual
- ctx.dev

---

## Wedge Ideas

What's the first thing that gets people in the door?

1. **Token Counter** — Free tool that counts/estimates tokens. Upsell to optimization.

2. **Context Playground** — Paste your context, see how different models handle it.

3. **RAG Evaluator** — Test your RAG pipeline quality. Identify gaps.

4. **Cost Calculator** — Estimate LLM costs based on context size.

---

## Questions to Answer

- [ ] Who is the exact first user? (Persona)
- [ ] What's the "aha moment"?
- [ ] Can we build something useful in 1 week?
- [ ] What's the minimum viable product?
- [ ] How do we validate demand before building?

---

## Research Needed

- [ ] How do power users currently manage context?
- [ ] What are the failure modes of RAG?
- [ ] Academic papers on context optimization?
- [ ] What prompts work best for summarization?

---

*Add your ideas here as they come up.*
